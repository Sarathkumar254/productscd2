name: Deploy to GCS

on:
  push:
    branches:
      - main

env:
  # set your bucket here (or reference another secret/env)
  BUCKET_NAME: spark-datasets-gds-123
  CODE_SUBDIR: products/py_modules

jobs:
  push-to-gcs:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v1
      with:
        # Service account key JSON, stored as a repo secret
        credentials_json: ${{ secrets.GCP_SA_KEY }}

    - name: Set up gcloud & gsutil
      uses: google-github-actions/setup-gcloud@v2
      with:
        project_id: ${{ secrets.GCP_PROJECT_ID }}
        # optional: install components
        install_components: gsutil

    - name: Download latest pydeequ wheel from PyPI
      run: |
        python3 -m pip install --upgrade pip
        mkdir -p wheels
        # --no-deps so we only pull the wheel itself
        pip download pydeequ --only-binary=:all: --no-deps -d wheels
        echo "Found wheel:"
        ls wheels

    - name: Upload pydeequ wheel to GCS
      run: |
        gsutil cp wheels/*.whl gs://$BUCKET_NAME/$CODE_SUBDIR/

    - name: Copy library modules to GCS
      run: |
        # create the target folder if you want (gsutil will auto-create)
        gsutil -m cp \
          src/deequ_checks.py \
          src/etl.py \
          src/spark_builder.py \
          src/writer.py \
          gs://$BUCKET_NAME/$CODE_SUBDIR/

    - name: Copy main.py to GCS root
      run: |
        gsutil cp src/main.py gs://$BUCKET_NAME/products/main.py